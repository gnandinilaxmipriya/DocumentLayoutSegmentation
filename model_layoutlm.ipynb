{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8da784c-c58c-4843-b845-7dd161eacc5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Found existing 'train_with_images' at /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/train_with_images\n",
      "[i] Found existing 'validation_with_images' at /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/validation_with_images\n",
      "[train] size: 300 columns: ['metadata', 'bboxes', 'category_id', 'segmentation', 'area', 'pdf_cells', 'image_path']\n",
      "[val]   size: 97 columns: ['metadata', 'bboxes', 'category_id', 'segmentation', 'area', 'pdf_cells', 'image_path']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0258ffea67df48aa8d2e9bf54e71ab93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfe291cf8f7454598528c149ac0ad03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/97 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483bbbd262724485bdd3baacbf0b8f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aaa8316b92044cab1cfc5df54cd6ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/97 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] kept 292 samples\n",
      "[val]   kept 96 samples\n",
      "skip reasons: {'no_image': 0, 'no_words': 9, 'len_mismatch': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-24251d5803/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='584' max='584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [584/584 06:39, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Token Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.012875</td>\n",
       "      <td>0.999907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.011723</td>\n",
       "      <td>0.999907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.011081</td>\n",
       "      <td>0.999907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.009590</td>\n",
       "      <td>0.999907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-24251d5803/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/jupyter-24251d5803/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/jupyter-24251d5803/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/jupyter-24251d5803/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/jupyter-24251d5803/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training finished (ALL classes).\n"
     ]
    }
   ],
   "source": [
    "# JUPYTER: LayoutLMv3 token classification on DocLayNet (ALL classes, with images)\n",
    "# Robust to nested pdf_cells; uses processor(images=..., text=...); skips safely; drops helper cols.\n",
    "import os, time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "from transformers import (\n",
    "    LayoutLMv3Processor,\n",
    "    LayoutLMv3ForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from PIL import Image\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_PATH = Path(\"/home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout\")\n",
    "IMAGE_ROOT = BASE_PATH / \"images\"   # images/{split}/{idx}.png\n",
    "\n",
    "MODEL_NAME   = \"microsoft/layoutlmv3-base\"\n",
    "OUTPUT_DIR   = \"./layoutlmv3_doclaynet_tokcls_all\"\n",
    "MAX_LEN      = 512\n",
    "LR           = 3e-5\n",
    "EPOCHS       = 4\n",
    "BATCH_TRAIN  = 2\n",
    "BATCH_EVAL   = 2\n",
    "\n",
    "# Labels: ALL DocLayNet classes + \"O\" (12 total)\n",
    "DOC_LAYNET_CLASSES = [\n",
    "    \"CAPTION\",\"FOOTNOTE\",\"FORMULA\",\"LIST-ITEM\",\"PAGE-FOOTER\",\n",
    "    \"PAGE-HEADER\",\"PICTURE\",\"SECTION-HEADER\",\"TABLE\",\"TEXT\",\"TITLE\"\n",
    "]\n",
    "LABELS   = [\"O\"] + DOC_LAYNET_CLASSES\n",
    "label2id = {l: i for i, l in enumerate(LABELS)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "ID2NAME_FALLBACK = {  # DocLayNet int ids → names\n",
    "    1:\"CAPTION\", 2:\"FOOTNOTE\", 3:\"FORMULA\", 4:\"LIST-ITEM\", 5:\"PAGE-FOOTER\",\n",
    "    6:\"PAGE-HEADER\", 7:\"PICTURE\", 8:\"SECTION-HEADER\", 9:\"TABLE\", 10:\"TEXT\", 11:\"TITLE\"\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# HELPERS — create {split}_with_images only if missing\n",
    "# =========================\n",
    "def get_metadata_keys(dataset) -> set:\n",
    "    return {tuple(sorted(meta.items())) for meta in dataset[\"metadata\"]}\n",
    "\n",
    "def save_image_file(pil_img: Image.Image, image_path: Path):\n",
    "    image_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    pil_img.save(image_path)\n",
    "\n",
    "def ensure_with_images_split(split: str):\n",
    "    with_images_dir = BASE_PATH / f\"{split}_with_images\"\n",
    "    if with_images_dir.exists():\n",
    "        print(f\"[i] Found existing '{split}_with_images' at {with_images_dir}\")\n",
    "        return\n",
    "    subset_dir = BASE_PATH / split\n",
    "    if not subset_dir.exists():\n",
    "        raise FileNotFoundError(f\"Subset not found: {subset_dir}\")\n",
    "    print(f\"[i] Creating '{split}_with_images' by attaching images...\")\n",
    "    ann_subset = load_from_disk(str(subset_dir))\n",
    "    meta_keys = get_metadata_keys(ann_subset)\n",
    "    if not meta_keys:\n",
    "        raise ValueError(f\"No 'metadata' column in {subset_dir}; cannot match images.\")\n",
    "    ann_dict = {tuple(sorted(meta.items())): i for i, meta in enumerate(ann_subset[\"metadata\"])}\n",
    "    image_paths = [None] * len(ann_subset)\n",
    "\n",
    "    t0 = time.time()\n",
    "    stream = load_dataset(\"ds4sd/DocLayNet-v1.2\", split=split, streaming=True)\n",
    "    matched = 0\n",
    "    for sample in stream:\n",
    "        key = tuple(sorted(sample[\"metadata\"].items()))\n",
    "        if key in meta_keys:\n",
    "            idx = ann_dict.get(key)\n",
    "            if idx is None:\n",
    "                continue\n",
    "            img_dir  = IMAGE_ROOT / split\n",
    "            img_path = img_dir / f\"{idx}.png\"\n",
    "            save_image_file(sample[\"image\"], img_path)\n",
    "            image_paths[idx] = str(img_path)\n",
    "            matched += 1\n",
    "            meta_keys.remove(key)\n",
    "            if not meta_keys:\n",
    "                break\n",
    "    print(f\"   ✅ Saved {matched} images to {IMAGE_ROOT/split} in {time.time()-t0:.1f}s\")\n",
    "    ann_subset = ann_subset.add_column(\"image_path\", image_paths)\n",
    "    ann_subset.save_to_disk(str(with_images_dir))\n",
    "    print(f\"   💾 Saved '{split}_with_images' at {with_images_dir}\")\n",
    "\n",
    "# Ensure with-images sets exist (you already have them; this won’t rebuild)\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = os.environ.get(\"HF_HUB_ENABLE_HF_TRANSFER\", \"1\")\n",
    "for split in [\"train\", \"validation\"]:\n",
    "    ensure_with_images_split(split)\n",
    "\n",
    "# =========================\n",
    "# GEOMETRY & LABELING\n",
    "# =========================\n",
    "def clamp(v, lo, hi): \n",
    "    return max(lo, min(hi, v))\n",
    "\n",
    "def to_1000_space(boxes: List[List[float]], w: int, h: int) -> List[List[int]]:\n",
    "    mv = max((max(b) for b in boxes if b), default=0.0)\n",
    "    out = []\n",
    "    for b in boxes:\n",
    "        if len(b) != 4:\n",
    "            out.append([0,0,1,1]); continue\n",
    "        x0,y0,x1,y1 = b\n",
    "        if x1 < x0: x0,x1 = x1,x0\n",
    "        if y1 < y0: y0,y1 = y1,y0\n",
    "        if mv <= 1.0001:\n",
    "            X0,Y0,X1,Y1 = int(round(x0*1000)),int(round(y0*1000)),int(round(x1*1000)),int(round(y1*1000))\n",
    "        elif w>0 and h>0:\n",
    "            X0,Y0,X1,Y1 = int(round((x0/w)*1000)),int(round((y0/h)*1000)),int(round((x1/w)*1000)),int(round((y1/h)*1000))\n",
    "        else:\n",
    "            X0,Y0,X1,Y1 = int(round(x0)),int(round(y0)),int(round(x1)),int(round(y1))\n",
    "        X0,Y0,X1,Y1 = clamp(X0,0,1000),clamp(Y0,0,1000),clamp(X1,0,1000),clamp(Y1,0,1000)\n",
    "        if X1 == X0: X1 = min(1000, X0+1)\n",
    "        if Y1 == Y0: Y1 = min(1000, Y0+1)\n",
    "        out.append([X0,Y0,X1,Y1])\n",
    "    return out\n",
    "\n",
    "def box_iou(a: List[float], b: List[float]) -> float:\n",
    "    ax0, ay0, ax1, ay1 = a\n",
    "    bx0, by0, bx1, by1 = b\n",
    "    ix0, iy0 = max(ax0, bx0), max(ay0, by0)\n",
    "    ix1, iy1 = min(ax1, bx1), min(ay1, by1)\n",
    "    iw, ih = max(0.0, ix1-ix0), max(0.0, iy1-iy0)\n",
    "    inter = iw*ih\n",
    "    areaA = max(0.0, ax1-ax0)*max(0.0, ay1-ay0)\n",
    "    areaB = max(0.0, bx1-bx0)*max(0.0, by1-by0)\n",
    "    union = areaA + areaB - inter + 1e-6\n",
    "    return inter/union\n",
    "\n",
    "def normalize_region_name(name: str) -> str:\n",
    "    return \"PICTURE\" if name == \"FIGURE\" else name\n",
    "\n",
    "def region_name_from_label(l: Any) -> str:\n",
    "    if isinstance(l, str): return normalize_region_name(l)\n",
    "    if isinstance(l, int): return normalize_region_name(ID2NAME_FALLBACK.get(l, \"TEXT\"))\n",
    "    return \"TEXT\"\n",
    "\n",
    "def label_id_from_name(name: str) -> int:\n",
    "    name = normalize_region_name(name)\n",
    "    if name not in DOC_LAYNET_CLASSES:\n",
    "        name = \"TEXT\"\n",
    "    return label2id[name]\n",
    "\n",
    "# Flatten nested pdf_cells → dicts with text & bbox\n",
    "def _iter_cells(obj: Any):\n",
    "    if obj is None:\n",
    "        return\n",
    "    if isinstance(obj, dict):\n",
    "        if \"text\" in obj and (\"bbox\" in obj or \"box\" in obj):\n",
    "            yield obj\n",
    "        for k in (\"cells\",\"items\",\"data\"):\n",
    "            if k in obj:\n",
    "                yield from _iter_cells(obj[k])\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        for it in obj:\n",
    "            yield from _iter_cells(it)\n",
    "\n",
    "def derive_words_from_pdf_cells(ex: Dict[str, Any]) -> Tuple[List[str], List[List[float]]]:\n",
    "    words, boxes = [], []\n",
    "    for c in _iter_cells(ex.get(\"pdf_cells\")):\n",
    "        bb  = c.get(\"bbox\", c.get(\"box\"))\n",
    "        txt = c.get(\"text\")\n",
    "        if not txt or not bb or len(bb) != 4: \n",
    "            continue\n",
    "        toks = str(txt).strip().split()\n",
    "        if not toks: \n",
    "            continue\n",
    "        words.extend(toks)\n",
    "        boxes.extend([bb]*len(toks))\n",
    "    return words, boxes\n",
    "\n",
    "def assign_cell_label(cell_box, region_boxes, region_labels) -> int:\n",
    "    if not region_boxes or not region_labels:\n",
    "        return 0\n",
    "    best, best_iou = -1, 0.0\n",
    "    for i, rb in enumerate(region_boxes):\n",
    "        iou = box_iou(cell_box, rb)\n",
    "        if iou > best_iou:\n",
    "            best_iou, best = iou, i\n",
    "    if best < 0:\n",
    "        return 0\n",
    "    rname = region_name_from_label(region_labels[best])\n",
    "    return label_id_from_name(rname)\n",
    "\n",
    "def detect_image(ex: Dict[str, Any]) -> Image.Image | None:\n",
    "    p = ex.get(\"image_path\")\n",
    "    if p:\n",
    "        try: return Image.open(p).convert(\"RGB\")\n",
    "        except Exception: return None\n",
    "    img = ex.get(\"image\")\n",
    "    if img is not None:\n",
    "        if isinstance(img, Image.Image): return img.convert(\"RGB\")\n",
    "        try: return Image.fromarray(img).convert(\"RGB\")\n",
    "        except Exception: return None\n",
    "    return None\n",
    "\n",
    "# =========================\n",
    "# PREPROCESS (dummy pack + images=/text=)\n",
    "# =========================\n",
    "def build_preprocess(processor):\n",
    "    skip = {\"no_image\":0, \"no_words\":0, \"len_mismatch\":0}\n",
    "\n",
    "    # dummy pack so skipped rows still emit full columns\n",
    "    _dummy_img = Image.new(\"RGB\", (8,8), color=(255,255,255))\n",
    "    _dummy_enc = processor(\n",
    "        images=_dummy_img, text=[\"_\"], boxes=[[0,0,1,1]], word_labels=[0],\n",
    "        truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\",\n",
    "    )\n",
    "    DUMMY_PACK = {k: v.squeeze(0) for k, v in _dummy_enc.items()}\n",
    "    DUMMY_PACK[\"_skip\"] = True\n",
    "\n",
    "    def preprocess(ex):\n",
    "        image = detect_image(ex)\n",
    "        if image is None:\n",
    "            skip[\"no_image\"] += 1\n",
    "            return DUMMY_PACK.copy()\n",
    "\n",
    "        # prefer existing perfectly aligned word-level data\n",
    "        words = ex.get(\"words\") or ex.get(\"tokens\") or []\n",
    "        word_boxes = ex.get(\"bboxes\") or []\n",
    "        direct_labels = ex.get(\"word_labels\") or ex.get(\"labels\")\n",
    "        ok_direct = bool(words) and bool(word_boxes) and bool(direct_labels) and \\\n",
    "                    len(words) == len(word_boxes) == len(direct_labels)\n",
    "\n",
    "        if not ok_direct:\n",
    "            words, word_boxes = derive_words_from_pdf_cells(ex)\n",
    "            if not words or not word_boxes:\n",
    "                skip[\"no_words\"] += 1\n",
    "                return DUMMY_PACK.copy()\n",
    "            if len(words) != len(word_boxes):\n",
    "                skip[\"len_mismatch\"] += 1\n",
    "                return DUMMY_PACK.copy()\n",
    "            region_boxes  = ex.get(\"bboxes\") or []\n",
    "            region_labels = ex.get(\"category_id\") or []\n",
    "            direct_labels = [assign_cell_label(bb, region_boxes, region_labels) for bb in word_boxes]\n",
    "\n",
    "        # normalize labels into ids 0..11 (0=\"O\")\n",
    "        norm = []\n",
    "        for l in direct_labels:\n",
    "            if isinstance(l, int):\n",
    "                if l == 0:\n",
    "                    norm.append(0)\n",
    "                elif 1 <= l <= 11:\n",
    "                    cname = ID2NAME_FALLBACK.get(l, \"TEXT\")\n",
    "                    norm.append(label_id_from_name(cname))\n",
    "                elif 0 <= l < len(LABELS):\n",
    "                    norm.append(l)\n",
    "                else:\n",
    "                    norm.append(0)\n",
    "            elif isinstance(l, str):\n",
    "                norm.append(label_id_from_name(l))\n",
    "            else:\n",
    "                norm.append(0)\n",
    "\n",
    "        # hard trim\n",
    "        if len(words) > MAX_LEN:\n",
    "            words = words[:MAX_LEN]; word_boxes = word_boxes[:MAX_LEN]; norm = norm[:MAX_LEN]\n",
    "\n",
    "        W, H = image.size\n",
    "        boxes_1000 = to_1000_space(word_boxes, W, H)\n",
    "\n",
    "        enc = processor(\n",
    "            images=image, text=words, boxes=boxes_1000, word_labels=norm,\n",
    "            truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\",\n",
    "        )\n",
    "        out = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        out[\"_skip\"] = False\n",
    "        return out\n",
    "\n",
    "    return preprocess, skip\n",
    "\n",
    "# =========================\n",
    "# LOAD, MAP, FILTER, CLEAN\n",
    "# =========================\n",
    "train_raw = load_from_disk(str(BASE_PATH / \"train_with_images\"))\n",
    "val_raw   = load_from_disk(str(BASE_PATH / \"validation_with_images\"))\n",
    "print(\"[train] size:\", len(train_raw), \"columns:\", train_raw.column_names)\n",
    "print(\"[val]   size:\", len(val_raw),   \"columns:\", val_raw.column_names)\n",
    "\n",
    "processor = LayoutLMv3Processor.from_pretrained(MODEL_NAME, apply_ocr=False)\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(LABELS), id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "preprocess, skip = build_preprocess(processor)\n",
    "train_ds = train_raw.map(preprocess, batched=False, remove_columns=train_raw.column_names)\n",
    "val_ds   = val_raw.map(preprocess,   batched=False, remove_columns=val_raw.column_names)\n",
    "\n",
    "train_ds = train_ds.filter(lambda ex: ex[\"_skip\"] is False)\n",
    "val_ds   = val_ds.filter(lambda ex: ex[\"_skip\"] is False)\n",
    "print(f\"[train] kept {len(train_ds)} samples\")\n",
    "print(f\"[val]   kept {len(val_ds)} samples\")\n",
    "print(\"skip reasons:\", skip)\n",
    "\n",
    "# drop helper + keep only model inputs\n",
    "if \"_skip\" in train_ds.column_names: train_ds = train_ds.remove_columns([\"_skip\"])\n",
    "if \"_skip\" in val_ds.column_names:   val_ds   = val_ds.remove_columns([\"_skip\"])\n",
    "MODEL_COLS = [\"input_ids\", \"attention_mask\", \"bbox\", \"pixel_values\", \"labels\"]\n",
    "train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in MODEL_COLS])\n",
    "val_ds   = val_ds.remove_columns([c for c in val_ds.column_names   if c not in MODEL_COLS])\n",
    "\n",
    "train_ds.set_format(type=\"torch\")\n",
    "val_ds.set_format(type=\"torch\")\n",
    "\n",
    "# =========================\n",
    "# TRAIN\n",
    "# =========================\n",
    "def compute_metrics(p):\n",
    "    # p.predictions, p.label_ids are NumPy arrays here\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    mask = labels != -100\n",
    "    if mask.any():\n",
    "        acc = (preds[mask] == labels[mask]).mean()  # NumPy mean -> float\n",
    "        return {\"token_acc\": float(acc)}\n",
    "    return {\"token_acc\": 0.0}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=processor.tokenizer,\n",
    "    padding=\"max_length\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "print(\"✅ Training finished (ALL classes).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be450c4-a43c-4d92-a068-a427a44106d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]\n",
      "numpy 1.26.4\n",
      "torch 2.9.0+cu128\n",
      "torchvision 0.24.0+cu128\n",
      "transformers 4.45.2\n",
      "datasets 4.2.0\n",
      "accelerate 1.10.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy, torch, torchvision\n",
    "\n",
    "import transformers, datasets\n",
    "import accelerate\n",
    "print(\"py\", sys.version)\n",
    "print(\"numpy\", numpy.__version__)\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"torchvision\", torchvision.__version__)\n",
    "\n",
    "\n",
    "\n",
    "print(\"transformers\", transformers.__version__)\n",
    "print(\"datasets\", datasets.__version__)\n",
    "print('accelerate', accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d48e7f66-0a64-4c04-938e-e7d053b0ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (4.2.0)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/tljh/user/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/tljh/user/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.local/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/tljh/user/lib/python3.10/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in ./.local/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.35.3)\n",
      "Requirement already satisfied: packaging in /opt/tljh/user/lib/python3.10/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.local/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: anyio in /opt/tljh/user/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: certifi in /opt/tljh/user/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/tljh/user/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/tljh/user/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.4)\n",
      "Requirement already satisfied: sniffio in /opt/tljh/user/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/tljh/user/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/tljh/user/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/tljh/user/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.15)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/tljh/user/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/tljh/user/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/tljh/user/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/tljh/user/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/tljh/user/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba051bc-0046-4999-b5f6-66772afded2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets==2.20.0\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers==4.45.2\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in ./.local/lib/python3.10/site-packages (21.0.0)\n",
      "Requirement already satisfied: timm>=0.9.12 in ./.local/lib/python3.10/site-packages (1.0.20)\n",
      "Collecting accelerate>=0.26\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from datasets==2.20.0) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/tljh/user/lib/python3.10/site-packages (from datasets==2.20.0) (1.26.4)\n",
      "Collecting pyarrow-hotfix (from datasets==2.20.0)\n",
      "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.20.0)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/tljh/user/lib/python3.10/site-packages (from datasets==2.20.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.local/lib/python3.10/site-packages (from datasets==2.20.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.local/lib/python3.10/site-packages (from datasets==2.20.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets==2.20.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.10/site-packages (from datasets==2.20.0) (0.70.16)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.10/site-packages (from datasets==2.20.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in ./.local/lib/python3.10/site-packages (from datasets==2.20.0) (0.35.3)\n",
      "Requirement already satisfied: packaging in /opt/tljh/user/lib/python3.10/site-packages (from datasets==2.20.0) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.10/site-packages (from datasets==2.20.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers==4.45.2) (2025.9.18)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers==4.45.2) (0.6.2)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.2)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (1.1.10)\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (from timm>=0.9.12) (2.9.0)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.10/site-packages (from timm>=0.9.12) (0.24.0)\n",
      "Requirement already satisfied: psutil in /opt/tljh/user/lib/python3.10/site-packages (from accelerate>=0.26) (5.9.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/tljh/user/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/tljh/user/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.20.0) (3.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/tljh/user/lib/python3.10/site-packages (from requests>=2.32.2->datasets==2.20.0) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.10/site-packages (from requests>=2.32.2->datasets==2.20.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.10/site-packages (from requests>=2.32.2->datasets==2.20.0) (2024.2.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.10/site-packages (from torch->timm>=0.9.12) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in ./.local/lib/python3.10/site-packages (from torch->timm>=0.9.12) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch->timm>=0.9.12) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.10/site-packages (from jinja2->torch->timm>=0.9.12) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/tljh/user/lib/python3.10/site-packages (from pandas->datasets==2.20.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/tljh/user/lib/python3.10/site-packages (from pandas->datasets==2.20.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/tljh/user/lib/python3.10/site-packages (from pandas->datasets==2.20.0) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/tljh/user/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.16.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.10/site-packages (from torchvision->timm>=0.9.12) (12.0.0)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: pyarrow-hotfix, fsspec, dill, tokenizers, transformers, accelerate, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec\n",
      "\u001b[2K    Found existing installation: fsspec 2025.9.0\n",
      "\u001b[2K    Uninstalling fsspec-2025.9.0:\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.9.0\n",
      "\u001b[2K  Attempting uninstall: dill[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: dill 0.4.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling dill-0.4.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled dill-0.4.0━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: tokenizers[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [dill]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.22.1━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [dill]\n",
      "\u001b[2K    Uninstalling tokenizers-0.22.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.22.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [dill]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/7\u001b[0m [transformers]\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jupyter-24251d5803/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m5/7\u001b[0m [accelerate]s]\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory, accelerate-launch and accelerate-merge-weights are installed in '/home/jupyter-24251d5803/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m6/7\u001b[0m [datasets]e]\u001b[33m  WARNING: The script datasets-cli is installed in '/home/jupyter-24251d5803/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.10.1 datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 pyarrow-hotfix-0.7 tokenizers-0.20.3 transformers-4.45.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"datasets==2.20.0\" \"transformers==4.45.2\" \"pyarrow>=14.0.1\" \"timm>=0.9.12\" \"accelerate>=0.26\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a79ba8f9-9cac-4694-afdc-68be599d9e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on dataset with 100 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869d48dc1d5f492ebf32f9d7a1abcfb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ace1cf7eef46a8b00b1cf7c8ad3d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset prepared: 0 samples, skipped: {'no_image': 0, 'no_words': 100, 'len_mismatch': 0}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'argmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 188\u001b[0m\n\u001b[1;32m    185\u001b[0m logits \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[1;32m    186\u001b[0m labels \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mlabel_ids\n\u001b[0;32m--> 188\u001b[0m pred_ids \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    189\u001b[0m mask \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    190\u001b[0m token_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m((pred_ids[mask] \u001b[38;5;241m==\u001b[39m labels[mask])\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'argmax'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Dict\n",
    "\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    LayoutLMv3Processor,\n",
    "    LayoutLMv3ForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from PIL import Image\n",
    "\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "# Paths - edit if needed\n",
    "BASE_PATH = Path(\"/home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout\")\n",
    "EVAL_DIR = BASE_PATH / \"test_with_images\"\n",
    "FALLBACK = BASE_PATH / \"validation_with_images\"\n",
    "CKPT_PATH = Path(\"./layoutlmv3_doclaynet_tokcls_all/checkpoint-584\")\n",
    "OUTPUT_EVAL = \"./eval_report_allclasses.txt\"\n",
    "\n",
    "# Label setup\n",
    "DOC_LAYNET_CLASSES = [\n",
    "    \"CAPTION\",\"FOOTNOTE\",\"FORMULA\",\"LIST-ITEM\",\"PAGE-FOOTER\",\n",
    "    \"PAGE-HEADER\",\"PICTURE\",\"SECTION-HEADER\",\"TABLE\",\"TEXT\",\"TITLE\"\n",
    "]\n",
    "LABELS = [\"O\"] + DOC_LAYNET_CLASSES\n",
    "label2id = {l: i for i, l in enumerate(LABELS)}\n",
    "id2label = {i: l for i, l in enumerate(LABELS)}\n",
    "ID2NAME_FALLBACK = {1:\"CAPTION\", 2:\"FOOTNOTE\", 3:\"FORMULA\", 4:\"LIST-ITEM\", 5:\"PAGE-FOOTER\",\n",
    "                    6:\"PAGE-HEADER\", 7:\"PICTURE\", 8:\"SECTION-HEADER\", 9:\"TABLE\", 10:\"TEXT\", 11:\"TITLE\"}\n",
    "\n",
    "MAX_LEN = 512\n",
    "BATCH_EVAL = 2\n",
    "\n",
    "# Helper functions (detect_image, to_1000_space, etc.) - Add your existing helper functions here\n",
    "\n",
    "def detect_image(ex: Dict[str, Any]) -> Image.Image | None:\n",
    "    p = ex.get(\"image_path\")\n",
    "    if p:\n",
    "        try: return Image.open(p).convert(\"RGB\")\n",
    "        except Exception: return None\n",
    "    img = ex.get(\"image\")\n",
    "    if img is not None:\n",
    "        if isinstance(img, Image.Image): return img.convert(\"RGB\")\n",
    "        try: return Image.fromarray(img).convert(\"RGB\")\n",
    "        except Exception: return None\n",
    "    return None\n",
    "\n",
    "def clamp(v, lo, hi): \n",
    "    return max(lo, min(hi, v))\n",
    "\n",
    "def to_1000_space(boxes: List[List[float]], w: int, h: int) -> List[List[int]]:\n",
    "    mv = max((max(b) for b in boxes if b), default=0.0)\n",
    "    out = []\n",
    "    for b in boxes:\n",
    "        if len(b) != 4:\n",
    "            out.append([0,0,1,1])\n",
    "            continue\n",
    "        x0,y0,x1,y1 = b\n",
    "        if x1 < x0: x0,x1 = x1,x0\n",
    "        if y1 < y0: y0,y1 = y1,y0\n",
    "        if mv <= 1.0001:\n",
    "            X0,Y0,X1,Y1 = int(round(x0*1000)),int(round(y0*1000)),int(round(x1*1000)),int(round(y1*1000))\n",
    "        elif w>0 and h>0:\n",
    "            X0,Y0,X1,Y1 = int(round((x0/w)*1000)),int(round((y0/h)*1000)),int(round((x1/w)*1000)),int(round((y1/h)*1000))\n",
    "        else:\n",
    "            X0,Y0,X1,Y1 = int(round(x0)),int(round(y0)),int(round(x1)),int(round(y1))\n",
    "        X0,Y0,X1,Y1 = clamp(X0,0,1000),clamp(Y0,0,1000),clamp(X1,0,1000),clamp(Y1,0,1000)\n",
    "        if X1 == X0: X1 = min(1000, X0+1)\n",
    "        if Y1 == Y0: Y1 = min(1000, Y0+1)\n",
    "        out.append([X0,Y0,X1,Y1])\n",
    "    return out\n",
    "\n",
    "# Align labels to tokens for proper evaluation ignoring subword tokens\n",
    "def align_labels_with_tokens(words: List[str], labels: List[int], tokenizer) -> List[int]:\n",
    "    tokenized = tokenizer(words, truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized.word_ids()\n",
    "    previous_word_idx = None\n",
    "    aligned_labels = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            aligned_labels.append(labels[word_idx])\n",
    "        else:\n",
    "            aligned_labels.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    return aligned_labels\n",
    "\n",
    "# Preprocess function for evaluation\n",
    "def build_preprocess(processor):\n",
    "    _dummy_img = Image.new(\"RGB\", (8,8), color=(255,255,255))\n",
    "    _dummy_enc = processor(images=_dummy_img, text=[\"_\"], boxes=[[0,0,1,1]], word_labels=[0],\n",
    "                           truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    DUMMY_PACK = {k: v.squeeze(0) for k, v in _dummy_enc.items()}\n",
    "    DUMMY_PACK[\"_skip\"] = True\n",
    "    skip = {\"no_image\":0, \"no_words\":0, \"len_mismatch\":0}\n",
    "\n",
    "    def preprocess(ex):\n",
    "        image = detect_image(ex)\n",
    "        if image is None:\n",
    "            skip[\"no_image\"] += 1\n",
    "            return DUMMY_PACK.copy()\n",
    "\n",
    "        words = ex.get(\"words\") or ex.get(\"tokens\") or []\n",
    "        word_boxes = ex.get(\"bboxes\") or []\n",
    "        direct_labels = ex.get(\"word_labels\") or ex.get(\"labels\")\n",
    "        if not (words and word_boxes and direct_labels and len(words) == len(word_boxes) == len(direct_labels)):\n",
    "            skip[\"no_words\"] += 1\n",
    "            return DUMMY_PACK.copy()\n",
    "\n",
    "        # Normalize labels\n",
    "        norm = []\n",
    "        for l in direct_labels:\n",
    "            if isinstance(l, int):\n",
    "                if 0 <= l < len(LABELS):\n",
    "                    norm.append(l)\n",
    "                else:\n",
    "                    norm.append(0)\n",
    "            elif isinstance(l, str):\n",
    "                norm.append(label2id.get(l, 0))\n",
    "            else:\n",
    "                norm.append(0)\n",
    "\n",
    "        if len(words) > MAX_LEN:\n",
    "            words, word_boxes, norm = words[:MAX_LEN], word_boxes[:MAX_LEN], norm[:MAX_LEN]\n",
    "\n",
    "        W, H = image.size\n",
    "        boxes_1000 = to_1000_space(word_boxes, W, H)\n",
    "\n",
    "        aligned_labels = align_labels_with_tokens(words, norm, processor.tokenizer)\n",
    "\n",
    "        enc = processor(images=image, text=words, boxes=boxes_1000, word_labels=aligned_labels,\n",
    "                        truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "        out = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        out[\"_skip\"] = False\n",
    "        return out\n",
    "\n",
    "    return preprocess, skip\n",
    "\n",
    "# Load eval dataset\n",
    "eval_raw = load_from_disk(str(EVAL_DIR)) if EVAL_DIR.exists() else load_from_disk(str(FALLBACK))\n",
    "print(f\"Evaluating on dataset with {len(eval_raw)} samples\")\n",
    "\n",
    "# Check checkpoint\n",
    "if not CKPT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {CKPT_PATH}\")\n",
    "\n",
    "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(CKPT_PATH).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "preprocess, skip = build_preprocess(processor)\n",
    "eval_ds = eval_raw.map(preprocess, batched=False, remove_columns=eval_raw.column_names)\n",
    "eval_ds = eval_ds.filter(lambda ex: ex[\"_skip\"] is False)\n",
    "if \"_skip\" in eval_ds.column_names: eval_ds = eval_ds.remove_columns([\"_skip\"])\n",
    "MODEL_COLS = [\"input_ids\", \"attention_mask\", \"bbox\", \"pixel_values\", \"labels\"]\n",
    "eval_ds = eval_ds.remove_columns([c for c in eval_ds.column_names if c not in MODEL_COLS])\n",
    "eval_ds.set_format(type=\"torch\")\n",
    "\n",
    "print(f\"Evaluation dataset prepared: {len(eval_ds)} samples, skipped: {skip}\")\n",
    "\n",
    "# Setup Trainer for prediction\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./tmp_eval_only\",\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=processor.tokenizer, padding=\"max_length\")\n",
    "\n",
    "eval_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "pred = eval_trainer.predict(eval_ds)\n",
    "logits = pred.predictions\n",
    "labels = pred.label_ids\n",
    "\n",
    "pred_ids = logits.argmax(-1)\n",
    "mask = labels != -100\n",
    "token_acc = float((pred_ids[mask] == labels[mask]).mean()) if mask.any() else 0.0\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "for logit, lab in zip(logits, labels):\n",
    "    pi = logit.argmax(-1)\n",
    "    m = lab != -100\n",
    "    y_true.append([id2label[int(i)] for i in lab[m]])\n",
    "    y_pred.append([id2label[int(i)] for i in pi[m]])\n",
    "\n",
    "report = classification_report(y_true, y_pred, digits=3)\n",
    "macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "micro_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "macro_p = precision_score(y_true, y_pred, average=\"macro\")\n",
    "macro_r = recall_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\n=== Evaluation Summary ===\")\n",
    "print(f\"Checkpoint: {CKPT_PATH}\")\n",
    "print(f\"Token accuracy: {token_acc:.4f}\")\n",
    "print(f\"Macro F1: {macro_f1:.4f} | Micro F1: {micro_f1:.4f} | Macro Precision: {macro_p:.4f} | Macro Recall: {macro_r:.4f}\")\n",
    "print(\"\\nPer-class report:\\n\", report)\n",
    "\n",
    "with open(OUTPUT_EVAL, \"w\") as f:\n",
    "    f.write(f\"Checkpoint: {CKPT_PATH}\\n\")\n",
    "    f.write(f\"Token accuracy: {token_acc:.6f}\\n\")\n",
    "    f.write(f\"Macro F1: {macro_f1:.6f} | Micro F1: {micro_f1:.6f} | Macro Precision: {macro_p:.6f} | Macro Recall: {macro_r:.6f}\\n\\n\")\n",
    "    f.write(report)\n",
    "print(f\"Report saved to: {OUTPUT_EVAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0ddf6e-dc73-4865-b9b0-8cac0c178e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script wheel is installed in '/home/jupyter-24251d5803/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pip setuptools wheel\n",
    "%pip install -qU --use-pep517 --no-build-isolation seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d4bfa6-d9f2-4a86-9da8-94f7f59b251f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No regions evaluated. Try lowering IOU_THRESH to 0.05 or check that eval_raw has 'bboxes'/'category_id'.\n"
     ]
    }
   ],
   "source": [
    "# ========= Region-level evaluation (majority vote over tokens per region) — FIXED =========\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "IOU_THRESH = 0.10  # try 0.05 if nothing overlaps\n",
    "\n",
    "ID2NAME_FALLBACK = {\n",
    "    1:\"CAPTION\", 2:\"FOOTNOTE\", 3:\"FORMULA\", 4:\"LIST-ITEM\", 5:\"PAGE-FOOTER\",\n",
    "    6:\"PAGE-HEADER\", 7:\"PICTURE\", 8:\"SECTION-HEADER\", 9:\"TABLE\", 10:\"TEXT\", 11:\"TITLE\"\n",
    "}\n",
    "\n",
    "def normalize_region_name(name: str) -> str:\n",
    "    return \"PICTURE\" if name == \"FIGURE\" else name\n",
    "\n",
    "def region_name_from_label(l):\n",
    "    if isinstance(l, str): return normalize_region_name(l)\n",
    "    if isinstance(l, int): return normalize_region_name(ID2NAME_FALLBACK.get(l, \"TEXT\"))\n",
    "    return \"TEXT\"\n",
    "\n",
    "def label_id_from_name(name: str) -> int:\n",
    "    name = normalize_region_name(name)\n",
    "    if name not in label2id:\n",
    "        name = \"TEXT\"\n",
    "    return label2id[name]\n",
    "\n",
    "def box_iou_1000(a, b) -> float:\n",
    "    ax0, ay0, ax1, ay1 = a\n",
    "    bx0, by0, bx1, by1 = b\n",
    "    ix0, iy0 = max(ax0, bx0), max(ay0, by0)\n",
    "    ix1, iy1 = min(ax1, bx1), min(ay1, by1)\n",
    "    iw, ih = max(0, ix1 - ix0), max(0, iy1 - iy0)\n",
    "    inter = iw * ih\n",
    "    areaA = max(0, ax1 - ax0) * max(0, ay1 - ay0)\n",
    "    areaB = max(0, bx1 - bx0) * max(0, by1 - by0)\n",
    "    union = areaA + areaB - inter + 1e-6\n",
    "    return inter / union\n",
    "\n",
    "region_true = []\n",
    "region_pred_majority = []       # majority including \"O\"\n",
    "region_pred_majority_nonO = []  # majority ignoring \"O\"\n",
    "\n",
    "N = len(eval_ds)\n",
    "for i in range(N):\n",
    "    ex = eval_raw[i]\n",
    "    gt_boxes_px   = ex.get(\"bboxes\") or []\n",
    "    gt_labels_raw = ex.get(\"category_id\") or []\n",
    "    if len(gt_boxes_px) == 0 or len(gt_labels_raw) == 0:\n",
    "        continue\n",
    "\n",
    "    # convert GT boxes to 0..1000 space\n",
    "    img = Image.open(ex[\"image_path\"]).convert(\"RGB\")\n",
    "    W, H = img.size\n",
    "    gt_boxes_1000 = to_1000_space(gt_boxes_px, W, H)\n",
    "    gt_label_ids  = [label_id_from_name(region_name_from_label(l)) for l in gt_labels_raw]\n",
    "\n",
    "    # predicted token labels for this sample\n",
    "    pred_ids = logits[i].argmax(-1)\n",
    "    lab_i    = labels[i]\n",
    "    mask     = (lab_i != -100)\n",
    "\n",
    "    token_boxes_1000 = np.array(eval_ds[i][\"bbox\"])[mask]\n",
    "    token_preds      = pred_ids[mask]\n",
    "\n",
    "    # vote per region\n",
    "    for r_box, r_true in zip(gt_boxes_1000, gt_label_ids):\n",
    "        hits = []\n",
    "        for tb, tpred in zip(token_boxes_1000, token_preds):\n",
    "            if box_iou_1000(tb, r_box) > IOU_THRESH:\n",
    "                hits.append(int(tpred))\n",
    "\n",
    "        if not hits:\n",
    "            maj = label2id[\"O\"]\n",
    "            maj_nonO = label2id[\"O\"]\n",
    "        else:\n",
    "            maj = Counter(hits).most_common(1)[0][0]\n",
    "            nonO = [h for h in hits if h != label2id[\"O\"]]\n",
    "            maj_nonO = Counter(nonO).most_common(1)[0][0] if nonO else label2id[\"O\"]\n",
    "\n",
    "        region_true.append(r_true)\n",
    "        region_pred_majority.append(maj)\n",
    "        region_pred_majority_nonO.append(maj_nonO)\n",
    "\n",
    "# If nothing got collected, exit early with a hint\n",
    "if len(region_true) == 0:\n",
    "    print(\"No regions evaluated. Try lowering IOU_THRESH to 0.05 or check that eval_raw has 'bboxes'/'category_id'.\")\n",
    "else:\n",
    "    # labels for report: **exclude 'O'** because GT regions never use it\n",
    "    class_names = [name for name, idx in sorted(label2id.items(), key=lambda x: x[1]) if name != \"O\"]\n",
    "    class_ids   = [label2id[name] for name in class_names]\n",
    "\n",
    "    print(\"\\n=== Region-level (majority vote, including 'O') ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(region_true, region_pred_majority))\n",
    "    print(classification_report(\n",
    "        region_true, region_pred_majority,\n",
    "        labels=class_ids, target_names=class_names, digits=3, zero_division=0\n",
    "    ))\n",
    "\n",
    "    print(\"\\n=== Region-level (majority vote, ignoring 'O' in the vote) ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(region_true, region_pred_majority_nonO))\n",
    "    print(classification_report(\n",
    "        region_true, region_pred_majority_nonO,\n",
    "        labels=class_ids, target_names=class_names, digits=3, zero_division=0\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25478912-c350-4012-ad85-f5afdec6eadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Loaded 100 samples from /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/test_with_images\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['metadata', 'bboxes', 'category_id', 'segmentation', 'area', 'pdf_cells', 'image_path']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 50\u001b[0m\n\u001b[1;32m     37\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForTokenClassification(\n\u001b[1;32m     38\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mtokenizer, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     42\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     43\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m_dummy_metrics\n\u001b[1;32m     48\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m logits \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[1;32m     52\u001b[0m labels \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mlabel_ids\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3946\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3943\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3945\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3946\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[1;32m   3948\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3949\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:4051\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4048\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4050\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 4051\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m   4052\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   4053\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   4054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/data_loader.py:567\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/data/data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/data/data_collator.py:333\u001b[0m, in \u001b[0;36mDataCollatorForTokenClassification.torch_call\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    329\u001b[0m labels \u001b[38;5;241m=\u001b[39m [feature[label_name] \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features] \u001b[38;5;28;01mif\u001b[39;00m label_name \u001b[38;5;129;01min\u001b[39;00m features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    331\u001b[0m no_labels_features \u001b[38;5;241m=\u001b[39m [{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m label_name} \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 333\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_labels_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3453\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3451\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has been passed for padding\u001b[39;00m\n\u001b[1;32m   3452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[0;32m-> 3453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3455\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3456\u001b[0m     )\n\u001b[1;32m   3458\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m   3460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['metadata', 'bboxes', 'category_id', 'segmentation', 'area', 'pdf_cells', 'image_path']"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "\n",
    "# ============== CONFIG ==============\n",
    "CKPT_PATH = Path(\"./layoutlmv3_doclaynet_tokcls_all/checkpoint-584\")\n",
    "EVAL_SPLIT = \"/home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/test_with_images\"  # change to validation if needed\n",
    "BATCH_SIZE = 2\n",
    "IOU_THRESH = 0.05  # lowered for region overlap\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ============== LOAD MODEL & PROCESSOR ==============\n",
    "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(CKPT_PATH).to(DEVICE)\n",
    "\n",
    "# ============== LOAD DATA ==============\n",
    "from datasets import load_from_disk\n",
    "eval_raw = load_from_disk(EVAL_SPLIT)\n",
    "eval_ds = eval_raw\n",
    "\n",
    "print(f\"[i] Loaded {len(eval_raw)} samples from {EVAL_SPLIT}\")\n",
    "\n",
    "# ============== PREDICT (logits + labels) ==============\n",
    "def _dummy_metrics(_): return {}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./eval_tmp\",\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=processor.tokenizer, padding=\"max_length\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=_dummy_metrics\n",
    ")\n",
    "\n",
    "pred = trainer.predict(eval_ds)\n",
    "logits = pred.predictions\n",
    "labels = pred.label_ids\n",
    "\n",
    "# ============== TOKEN-LEVEL EVALUATION ==============\n",
    "pred_ids = logits.argmax(-1)\n",
    "mask = labels != -100\n",
    "\n",
    "token_acc = float((pred_ids[mask] == labels[mask]).mean()) if mask.any() else 0.0\n",
    "print(f\"\\n✅ Token Accuracy: {token_acc:.4f}\")\n",
    "\n",
    "# build token-level true/pred label lists excluding -100 and 'O'\n",
    "y_true, y_pred = [], []\n",
    "for lrow, prow in zip(labels, pred_ids):\n",
    "    for l, p in zip(lrow, prow):\n",
    "        if l == -100: continue\n",
    "        y_true.append(int(l))\n",
    "        y_pred.append(int(p))\n",
    "\n",
    "# prepare class names (excluding O if needed)\n",
    "id2label = model.config.id2label\n",
    "label2id = {v: int(k) for k, v in model.config.label2id.items()}\n",
    "class_names = [id2label[str(i)] for i in range(len(id2label))]\n",
    "non_o_ids = [i for i, name in enumerate(class_names) if name != \"O\"]\n",
    "non_o_names = [name for name in class_names if name != \"O\"]\n",
    "\n",
    "print(\"\\n=== TOKEN-LEVEL REPORT (excluding 'O') ===\")\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    labels=non_o_ids,\n",
    "    target_names=non_o_names,\n",
    "    digits=3,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# Confusion matrix visualization\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_true, y_pred, labels=non_o_ids)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=non_o_names)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp.plot(ax=ax, xticks_rotation='vertical', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Token Level, Non-O)\")\n",
    "plt.show()\n",
    "\n",
    "# ============== REGION-LEVEL EVALUATION ==============\n",
    "def box_iou_1000(a, b):\n",
    "    ax0, ay0, ax1, ay1 = a\n",
    "    bx0, by0, bx1, by1 = b\n",
    "    ix0, iy0 = max(ax0, bx0), max(ay0, by0)\n",
    "    ix1, iy1 = min(ax1, bx1), min(ay1, by1)\n",
    "    iw, ih = max(0, ix1 - ix0), max(0, iy1 - iy0)\n",
    "    inter = iw * ih\n",
    "    areaA = max(0, ax1 - ax0) * max(0, ay1 - ay0)\n",
    "    areaB = max(0, bx1 - bx0) * max(0, by1 - by0)\n",
    "    union = areaA + areaB - inter + 1e-6\n",
    "    return inter / union\n",
    "\n",
    "region_true, region_pred = [], []\n",
    "\n",
    "for i in range(len(eval_raw)):\n",
    "    ex = eval_raw[i]\n",
    "    if not ex.get(\"bboxes\") or not ex.get(\"category_id\"):\n",
    "        continue\n",
    "\n",
    "    # GT regions\n",
    "    img = Image.open(ex[\"image_path\"]).convert(\"RGB\")\n",
    "    W, H = img.size\n",
    "    gt_boxes = np.array([[b[0]*1000/W, b[1]*1000/H, b[2]*1000/W, b[3]*1000/H] for b in ex[\"bboxes\"]])\n",
    "    gt_labels = [int(l) if isinstance(l, int) else label2id[l] for l in ex[\"category_id\"]]\n",
    "\n",
    "    # Token-level predictions for this page\n",
    "    token_boxes = np.array(eval_ds[i][\"bbox\"])\n",
    "    page_pred = logits[i].argmax(-1)\n",
    "    page_mask = labels[i] != -100\n",
    "    token_boxes = token_boxes[page_mask]\n",
    "    page_pred = page_pred[page_mask]\n",
    "\n",
    "    # vote per region\n",
    "    for r_box, r_label in zip(gt_boxes, gt_labels):\n",
    "        hits = [int(pred) for tbox, pred in zip(token_boxes, page_pred)\n",
    "                if box_iou_1000(tbox, r_box) > IOU_THRESH]\n",
    "        if not hits:\n",
    "            region_pred.append(label2id[\"O\"])\n",
    "        else:\n",
    "            region_pred.append(Counter(hits).most_common(1)[0][0])\n",
    "        region_true.append(r_label)\n",
    "\n",
    "if len(region_true) == 0:\n",
    "    print(\"\\n❌ No regions evaluated. Check your eval_raw['bboxes'] or IOU threshold.\")\n",
    "else:\n",
    "    print(\"\\n=== REGION-LEVEL REPORT (majority vote) ===\")\n",
    "    print(classification_report(\n",
    "        region_true, region_pred,\n",
    "        labels=non_o_ids,\n",
    "        target_names=non_o_names,\n",
    "        digits=3,\n",
    "        zero_division=0\n",
    "    ))\n",
    "    print(\"Region-level Accuracy:\", accuracy_score(region_true, region_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5777c717-9559-4e5e-ad13-6d08e3945088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region (page-level) counts:\n",
      "          TEXT: 593\n",
      "     LIST-ITEM: 394\n",
      "SECTION-HEADER: 385\n",
      "       PICTURE: 278\n",
      "         TABLE: 232\n",
      "   PAGE-FOOTER: 205\n",
      "       CAPTION: 158\n",
      "   PAGE-HEADER: 140\n",
      "      FOOTNOTE: 136\n",
      "         TITLE: 49\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = Path(\"/home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout\")\n",
    "ID2NAME_FALLBACK = {\n",
    "    1:\"CAPTION\",2:\"FOOTNOTE\",3:\"FORMULA\",4:\"LIST-ITEM\",5:\"PAGE-FOOTER\",\n",
    "    6:\"PAGE-HEADER\",7:\"PICTURE\",8:\"SECTION-HEADER\",9:\"TABLE\",10:\"TEXT\",11:\"TITLE\"\n",
    "}\n",
    "\n",
    "train_raw = load_from_disk(str(BASE_PATH / \"train_with_images\"))\n",
    "region_counts = Counter()\n",
    "for ex in train_raw:\n",
    "    for lab in ex.get(\"category_id\", []):\n",
    "        region_counts[ID2NAME_FALLBACK.get(int(lab), \"TEXT\")] += 1\n",
    "\n",
    "print(\"Region (page-level) counts:\")\n",
    "for k,v in region_counts.most_common():\n",
    "    print(f\"{k:>14}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0f4ce-103e-4c66-9ca9-3132cca81669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
