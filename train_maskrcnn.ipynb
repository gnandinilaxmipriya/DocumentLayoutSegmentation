{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ef1eae-c5bb-4365-b8ea-b33ad064078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in ./.local/lib/python3.10/site-packages (25.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.10/site-packages (0.24.0)\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/tljh/user/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.local/lib/python3.10/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.local/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.local/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.local/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.local/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.local/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.local/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.local/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in ./.local/lib/python3.10/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: numpy in /opt/tljh/user/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.10/site-packages (from torchvision) (12.0.0)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading pycocotools-2.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (455 kB)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, pycocotools, opencv-python\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [numpy]\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/home/jupyter-24251d5803/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [opencv-python]0m [opencv-python]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.2.6 opencv-python-4.12.0.88 pycocotools-2.0.10\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch torchvision pycocotools opencv-python tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b58dfc-d604-416d-9c29-eb9f683bd0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement detectron2==0.6 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for detectron2==0.6\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install detectron2==0.6 opencv-python pycocotools tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4edea601-147e-4334-912b-3ded22b279cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train exists: True\n",
      "train_with_images exists: True\n",
      "validation_with_images exists: True\n",
      "columns: ['metadata', 'bboxes', 'category_id', 'segmentation', 'area', 'pdf_cells', 'image_path']\n",
      "0 /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/images/train/0.png\n",
      "1 /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/images/train/1.png\n",
      "2 /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/images/train/2.png\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_from_disk\n",
    "\n",
    "BASE = Path(\"/home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout\")  # << adjust if needed\n",
    "IMG_ROOT = BASE / \"images\"   # images/train/{i}.png etc.\n",
    "\n",
    "# you already have these\n",
    "print(\"train exists:\", (BASE/\"train\").exists())\n",
    "print(\"train_with_images exists:\", (BASE/\"train_with_images\").exists())\n",
    "print(\"validation_with_images exists:\", (BASE/\"validation_with_images\").exists())\n",
    "\n",
    "# peek a couple of rows to confirm image paths are mapped\n",
    "if (BASE/\"train_with_images\").exists():\n",
    "    ds = load_from_disk(str(BASE/\"train_with_images\"))\n",
    "    print(\"columns:\", ds.column_names)\n",
    "    for i in range(min(3, len(ds))):\n",
    "        print(i, ds[i].get(\"image_path\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff02c586-47ab-4e9a-a362-8b55d9c4284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COCO train: 100%|██████████| 300/300 [00:01<00:00, 205.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ wrote /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/train.json  | images=300  anns=1261\n",
      "   uses image root: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/images/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COCO validation: 100%|██████████| 97/97 [00:00<00:00, 177.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ wrote /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/validation.json  | images=97  anns=521\n",
      "   uses image root: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/images/validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "OUT = BASE / \"MaskRCNN_torchvision\"   # where COCO jsons + model outputs will live\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGETS = [\"TEXT\", \"TABLE\", \"PICTURE\", \"CAPTION\"]\n",
    "CAT2ID  = {c:i+1 for i,c in enumerate(TARGETS)}  # COCO category ids 1..4\n",
    "\n",
    "ID2NAME_FALLBACK = {\n",
    "    1:\"CAPTION\", 2:\"FOOTNOTE\", 3:\"FORMULA\", 4:\"LIST-ITEM\", 5:\"PAGE-FOOTER\",\n",
    "    6:\"PAGE-HEADER\", 7:\"PICTURE\", 8:\"SECTION-HEADER\", 9:\"TABLE\", 10:\"TEXT\", 11:\"TITLE\"\n",
    "}\n",
    "\n",
    "def norm_name(name_or_id):\n",
    "    \"\"\"Map DocLayNet label (int or str) → our 4 targets; FIGURE→PICTURE.\"\"\"\n",
    "    if isinstance(name_or_id, int):\n",
    "        name = ID2NAME_FALLBACK.get(name_or_id, \"TEXT\")\n",
    "    else:\n",
    "        name = str(name_or_id)\n",
    "    if name == \"FIGURE\":\n",
    "        name = \"PICTURE\"\n",
    "    return name\n",
    "\n",
    "def to_xywh(box, W, H):\n",
    "    \"\"\"Accept [x0,y0,x1,y1] in pixels or normalized; return pixel [x,y,w,h].\"\"\"\n",
    "    if not box or len(box) != 4: return None\n",
    "    x0,y0,x1,y1 = map(float, box)\n",
    "    if max(abs(x0),abs(y0),abs(x1),abs(y1)) <= 1.05 and W>1 and H>1:\n",
    "        x0,y0,x1,y1 = x0*W, y0*H, x1*W, y1*H\n",
    "    x0,x1 = min(x0,x1), max(x0,x1)\n",
    "    y0,y1 = min(y0,y1), max(y0,y1)\n",
    "    x0 = max(0, min(W-1, x0)); y0 = max(0, min(H-1, y0))\n",
    "    x1 = max(1, min(W,   x1)); y1 = max(1, min(H,   y1))\n",
    "    w  = max(1, x1 - x0); h = max(1, y1 - y0)\n",
    "    return [float(x0), float(y0), float(w), float(h)]\n",
    "\n",
    "def build_coco_for_split(split_with_images: str):\n",
    "    \"\"\"split_with_images: 'train_with_images' or 'validation_with_images'\"\"\"\n",
    "    ds_dir = BASE / split_with_images\n",
    "    ds = load_from_disk(str(ds_dir))\n",
    "\n",
    "    split = split_with_images.replace(\"_with_images\",\"\")\n",
    "    images_dir = IMG_ROOT / split\n",
    "\n",
    "    coco = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": CAT2ID[name], \"name\": name} for name in TARGETS]\n",
    "    }\n",
    "\n",
    "    ann_id = 1\n",
    "    for img_id, ex in enumerate(tqdm(ds, desc=f\"COCO {split}\"), start=1):\n",
    "        img_path = ex.get(\"image_path\") or str(images_dir / f\"{img_id-1}.png\")\n",
    "        try:\n",
    "            with Image.open(img_path) as im:\n",
    "                W, H = im.size\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        coco[\"images\"].append({\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": os.path.basename(img_path),\n",
    "            \"width\": W,\n",
    "            \"height\": H\n",
    "        })\n",
    "\n",
    "        boxes = ex.get(\"bboxes\") or []\n",
    "        cats  = ex.get(\"category_id\") or []\n",
    "        for b, c in zip(boxes, cats):\n",
    "            cname = norm_name(c)\n",
    "            if cname not in TARGETS:\n",
    "                continue\n",
    "            bbox = to_xywh(b, W, H)\n",
    "            if bbox is None:\n",
    "                continue\n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": ann_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": CAT2ID[cname],\n",
    "                \"bbox\": bbox,\n",
    "                \"area\": float(bbox[2] * bbox[3]),\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            ann_id += 1\n",
    "\n",
    "    out_json = OUT / f\"{split}.json\"\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(coco, f)\n",
    "    print(f\"✔ wrote {out_json}  | images={len(coco['images'])}  anns={len(coco['annotations'])}\")\n",
    "    print(f\"   uses image root: {images_dir}\")\n",
    "    return str(out_json), str(images_dir)\n",
    "\n",
    "train_json, train_imgdir = build_coco_for_split(\"train_with_images\")\n",
    "val_json,   val_imgdir   = build_coco_for_split(\"validation_with_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0789109a-492b-4f51-9db4-982c9e339791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(imgs), \u001b[38;5;28mlist\u001b[39m(tgts)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# loaders\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m COCOSimple(\u001b[43mtrain_json\u001b[49m, train_imgdir)\n\u001b[1;32m     80\u001b[0m val_ds   \u001b[38;5;241m=\u001b[39m COCOSimple(val_json,   val_imgdir)\n\u001b[1;32m     81\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_json' is not defined"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np, torch, torchvision, random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "TARGETS = [\"TEXT\",\"TABLE\",\"PICTURE\",\"CAPTION\"]\n",
    "NUM_CLASSES = 1 + len(TARGETS)   # background + 4\n",
    "\n",
    "# ---------- minimal COCO dataset ----------\n",
    "class COCOSimple(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      image (FloatTensor [C,H,W])\n",
    "      target dict: boxes [N,4] (xyxy), labels [N], masks [N,H,W], image_id\n",
    "    Masks are rectangles derived from boxes (simple but effective for mask head).\n",
    "    \"\"\"\n",
    "    def __init__(self, json_path, images_dir):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            coco = json.load(f)\n",
    "        self.images_dir = images_dir\n",
    "        self.imgs = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "        self.by_img = {img_id: [] for img_id in self.imgs}\n",
    "        for ann in coco[\"annotations\"]:\n",
    "            self.by_img[ann[\"image_id\"]].append(ann)\n",
    "        self.ids = list(self.imgs.keys())\n",
    "\n",
    "    def __len__(self): return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        meta = self.imgs[img_id]\n",
    "        path = os.path.join(self.images_dir, meta[\"file_name\"])\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        W, H = img.size\n",
    "\n",
    "        anns = self.by_img.get(img_id, [])\n",
    "        boxes, labels, masks = [], [], []\n",
    "\n",
    "        for a in anns:\n",
    "            x,y,w,h = a[\"bbox\"]\n",
    "            if w <= 1 or h <= 1: \n",
    "                continue\n",
    "            x0,y0,x1,y1 = x, y, x+w, y+h\n",
    "            boxes.append([x0,y0,x1,y1])\n",
    "            labels.append(int(a[\"category_id\"]))\n",
    "            m = np.zeros((H,W), dtype=np.uint8)\n",
    "            m[int(y0):int(y1), int(x0):int(x1)] = 1\n",
    "            masks.append(m)\n",
    "\n",
    "        if len(boxes)==0:\n",
    "            boxes  = [[0.0,0.0,1.0,1.0]]\n",
    "            labels = [0]\n",
    "            masks  = [np.zeros((H,W), dtype=np.uint8)]\n",
    "\n",
    "        boxes  = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        masks  = torch.as_tensor(np.stack(masks,0), dtype=torch.uint8)\n",
    "        image  = TF.to_tensor(img)  # [0,1] float32\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes, \"labels\": labels, \"masks\": masks,\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "            \"iscrowd\": torch.zeros((boxes.shape[0],), dtype=torch.int64),\n",
    "            \"area\": (boxes[:,2]-boxes[:,0])*(boxes[:,3]-boxes[:,1]),\n",
    "        }\n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, tgts = list(zip(*batch))\n",
    "    return list(imgs), list(tgts)\n",
    "\n",
    "# loaders\n",
    "train_ds = COCOSimple(train_json, train_imgdir)\n",
    "val_ds   = COCOSimple(val_json,   val_imgdir)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True,  num_workers=2, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "# model\n",
    "model = maskrcnn_resnet50_fpn(weights=\"COCO_V1\")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "# replace heads\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, NUM_CLASSES)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=2.5e-4, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "\n",
    "EPOCHS = 10\n",
    "save_dir = OUT / \"tv_maskrcnn_output\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    tr_loss = 0.0\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Train {epoch+1}/{EPOCHS}\"):\n",
    "        images  = [im.to(device) for im in images]\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "    lr_scheduler.step()\n",
    "    tr_loss /= max(1, len(train_loader))\n",
    "\n",
    "    # ---- val (loss) ----\n",
    "    model.train()  # to get detection losses\n",
    "    va_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc=\"Val\"):\n",
    "            images  = [im.to(device) for im in images]\n",
    "            targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "            ldict = model(images, targets)\n",
    "            va_loss += sum(ldict.values()).item()\n",
    "    va_loss /= max(1, len(val_loader))\n",
    "    print(f\"Epoch {epoch+1}: train_loss={tr_loss:.4f}  val_loss={va_loss:.4f}\")\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        torch.save(model.state_dict(), str(save_dir/\"model_best.pth\"))\n",
    "        print(\"✔ saved best:\", save_dir/\"model_best.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), str(save_dir/\"model_last.pth\"))\n",
    "print(\"✔ training complete →\", save_dir/\"model_last.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1f6d8b1-f337-4a52-ab32-2deb1ccfdb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/30: 100%|██████████| 150/150 [00:36<00:00,  4.07it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=1.5172  val_loss=1.3520\n",
      "✅ Saved best: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/tv_maskrcnn_output_e30/model_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2/30: 100%|██████████| 150/150 [00:38<00:00,  3.90it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss=1.2448  val_loss=1.2335\n",
      "✅ Saved best: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/tv_maskrcnn_output_e30/model_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3/30: 100%|██████████| 150/150 [00:39<00:00,  3.84it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss=1.2216  val_loss=1.2674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4/30: 100%|██████████| 150/150 [00:39<00:00,  3.84it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss=1.1787  val_loss=1.2181\n",
      "✅ Saved best: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/tv_maskrcnn_output_e30/model_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 5/30: 100%|██████████| 150/150 [00:39<00:00,  3.80it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_loss=1.1437  val_loss=1.2044\n",
      "✅ Saved best: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/tv_maskrcnn_output_e30/model_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 6/30: 100%|██████████| 150/150 [00:38<00:00,  3.93it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_loss=1.1571  val_loss=1.2691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 7/30: 100%|██████████| 150/150 [00:39<00:00,  3.84it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train_loss=1.1374  val_loss=1.2502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 8/30: 100%|██████████| 150/150 [00:39<00:00,  3.77it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train_loss=1.1074  val_loss=1.2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 9/30: 100%|██████████| 150/150 [00:39<00:00,  3.75it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train_loss=1.0338  val_loss=1.1907\n",
      "✅ Saved best: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/tv_maskrcnn_output_e30/model_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 10/30: 100%|██████████| 150/150 [00:39<00:00,  3.80it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train_loss=0.9967  val_loss=1.1619\n",
      "✅ Saved best: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/tv_maskrcnn_output_e30/model_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 11/30: 100%|██████████| 150/150 [00:40<00:00,  3.74it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train_loss=0.9676  val_loss=1.1644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 12/30: 100%|██████████| 150/150 [00:39<00:00,  3.83it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train_loss=0.9586  val_loss=1.1734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 13/30: 100%|██████████| 150/150 [00:39<00:00,  3.80it/s]\n",
      "Val: 100%|██████████| 49/49 [00:07<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train_loss=0.9517  val_loss=1.1761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 14/30: 100%|██████████| 150/150 [00:39<00:00,  3.76it/s]\n",
      "Val: 100%|██████████| 49/49 [00:07<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train_loss=0.9296  val_loss=1.1816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 15/30: 100%|██████████| 150/150 [00:39<00:00,  3.78it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train_loss=0.9219  val_loss=1.1993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 16/30: 100%|██████████| 150/150 [00:40<00:00,  3.72it/s]\n",
      "Val: 100%|██████████| 49/49 [00:07<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: train_loss=0.9040  val_loss=1.2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 17/30: 100%|██████████| 150/150 [00:39<00:00,  3.77it/s]\n",
      "Val: 100%|██████████| 49/49 [00:07<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train_loss=0.8939  val_loss=1.1898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 18/30: 100%|██████████| 150/150 [00:40<00:00,  3.71it/s]\n",
      "Val: 100%|██████████| 49/49 [00:07<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: train_loss=0.8806  val_loss=1.1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 19/30: 100%|██████████| 150/150 [00:40<00:00,  3.71it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: train_loss=0.8693  val_loss=1.1943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 20/30: 100%|██████████| 150/150 [00:40<00:00,  3.72it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: train_loss=0.8750  val_loss=1.1938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 21/30: 100%|██████████| 150/150 [00:40<00:00,  3.71it/s]\n",
      "Val: 100%|██████████| 49/49 [00:07<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: train_loss=0.8757  val_loss=1.2082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 22/30: 100%|██████████| 150/150 [00:40<00:00,  3.71it/s]\n",
      "Val: 100%|██████████| 49/49 [00:07<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: train_loss=0.8738  val_loss=1.1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 23/30: 100%|██████████| 150/150 [00:40<00:00,  3.71it/s]\n",
      "Val: 100%|██████████| 49/49 [00:07<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: train_loss=0.8737  val_loss=1.2099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 24/30: 100%|██████████| 150/150 [00:40<00:00,  3.71it/s]\n",
      "Val: 100%|██████████| 49/49 [00:07<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: train_loss=0.8701  val_loss=1.2068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 25/30: 100%|██████████| 150/150 [00:39<00:00,  3.75it/s]\n",
      "Val: 100%|██████████| 49/49 [00:07<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: train_loss=0.8599  val_loss=1.2146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 26/30: 100%|██████████| 150/150 [00:40<00:00,  3.70it/s]\n",
      "Val: 100%|██████████| 49/49 [00:07<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: train_loss=0.8632  val_loss=1.2117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 27/30: 100%|██████████| 150/150 [00:40<00:00,  3.73it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: train_loss=0.8640  val_loss=1.2137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 28/30: 100%|██████████| 150/150 [00:40<00:00,  3.71it/s]\n",
      "Val: 100%|██████████| 49/49 [00:07<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: train_loss=0.8639  val_loss=1.2080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 29/30: 100%|██████████| 150/150 [00:41<00:00,  3.64it/s]\n",
      "Val: 100%|██████████| 49/49 [00:06<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: train_loss=0.8542  val_loss=1.2123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 30/30: 100%|██████████| 150/150 [00:40<00:00,  3.73it/s]\n",
      "Val: 100%|██████████| 49/49 [00:07<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: train_loss=0.8603  val_loss=1.2106\n",
      "✔ Training complete → /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/tv_maskrcnn_output_e30/model_last.pth\n"
     ]
    }
   ],
   "source": [
    "# === 30-epoch Mask R-CNN training (torchvision), self-contained ===\n",
    "import os, json, numpy as np, torch, random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "# ---- Paths (adjust BASE if needed) ----\n",
    "BASE = Path(\"/home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout\")\n",
    "OUT  = BASE / \"MaskRCNN_torchvision\"\n",
    "train_json   = str(OUT / \"train.json\")\n",
    "val_json     = str(OUT / \"validation.json\")\n",
    "train_imgdir = str(BASE / \"images\" / \"train\")\n",
    "val_imgdir   = str(BASE / \"images\" / \"validation\")\n",
    "\n",
    "assert os.path.isfile(train_json), f\"Missing {train_json}. Run the COCO conversion step first.\"\n",
    "assert os.path.isfile(val_json),   f\"Missing {val_json}. Run the COCO conversion step first.\"\n",
    "\n",
    "# ---- Config ----\n",
    "TARGETS = [\"TEXT\",\"TABLE\",\"PICTURE\",\"CAPTION\"]\n",
    "NUM_CLASSES = 1 + len(TARGETS)  # background + 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# ---- Minimal COCO dataset with light augments for train ----\n",
    "# --- DROP-IN: robust dataset with safe flip + bbox sanitize ---\n",
    "import os, json, random, numpy as np, torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class COCOSimple(Dataset):\n",
    "    def __init__(self, json_path, img_root, is_train=False):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            coco = json.load(f)\n",
    "        self.img_root = img_root\n",
    "        self.images = {im[\"id\"]: im for im in coco[\"images\"]}\n",
    "        self.anns_by_img = {imid: [] for imid in self.images}\n",
    "        for a in coco[\"annotations\"]:\n",
    "            self.anns_by_img[a[\"image_id\"]].append(a)\n",
    "        self.ids = list(self.images.keys())\n",
    "        self.is_train = is_train  # apply augments only on train\n",
    "\n",
    "    def _sanitize(self, boxes, labels, masks, W, H, min_size=1.0):\n",
    "        \"\"\"\n",
    "        boxes: Tensor [N,4] (xyxy), labels: Tensor [N], masks: Tensor [N,H,W] (uint8)\n",
    "        Ensures x0<=x1, y0<=y1, clamps to image bounds, removes degenerate boxes.\n",
    "        \"\"\"\n",
    "        if boxes.numel() == 0:\n",
    "            return boxes, labels, masks\n",
    "\n",
    "        # clamp to bounds (allow x1==W, y1==H so width/height stay positive after max with min_size)\n",
    "        boxes[:, 0] = boxes[:, 0].clamp(0, W)\n",
    "        boxes[:, 2] = boxes[:, 2].clamp(0, W)\n",
    "        boxes[:, 1] = boxes[:, 1].clamp(0, H)\n",
    "        boxes[:, 3] = boxes[:, 3].clamp(0, H)\n",
    "\n",
    "        # ensure proper ordering\n",
    "        x0 = torch.minimum(boxes[:, 0], boxes[:, 2])\n",
    "        x1 = torch.maximum(boxes[:, 0], boxes[:, 2])\n",
    "        y0 = torch.minimum(boxes[:, 1], boxes[:, 3])\n",
    "        y1 = torch.maximum(boxes[:, 1], boxes[:, 3])\n",
    "        boxes = torch.stack([x0, y0, x1, y1], dim=1)\n",
    "\n",
    "        # enforce strictly positive width/height\n",
    "        w = boxes[:, 2] - boxes[:, 0]\n",
    "        h = boxes[:, 3] - boxes[:, 1]\n",
    "        keep = (w > min_size) & (h > min_size)\n",
    "\n",
    "        if keep.any():\n",
    "            boxes  = boxes[keep]\n",
    "            labels = labels[keep]\n",
    "            masks  = masks[keep] if masks is not None and masks.numel() > 0 else masks\n",
    "        else:\n",
    "            # if everything got filtered, return one tiny background box to avoid crashes\n",
    "            boxes  = torch.tensor([[0.0, 0.0, 1.0, 1.0]], dtype=torch.float32)\n",
    "            labels = torch.tensor([0], dtype=torch.int64)\n",
    "            masks  = torch.zeros((1, H, W), dtype=torch.uint8)\n",
    "\n",
    "        return boxes, labels, masks\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im_id = self.ids[idx]\n",
    "        meta  = self.images[im_id]\n",
    "        path  = os.path.join(self.img_root, meta[\"file_name\"])\n",
    "        img   = Image.open(path).convert(\"RGB\")\n",
    "        W, H  = img.size\n",
    "\n",
    "        # build targets from COCO (xywh -> xyxy)\n",
    "        anns = self.anns_by_img.get(im_id, [])\n",
    "        boxes, labels, masks = [], [], []\n",
    "        for a in anns:\n",
    "            x, y, w, h = a[\"bbox\"]\n",
    "            if w <= 1 or h <= 1:\n",
    "                continue\n",
    "            x0, y0, x1, y1 = x, y, x + w, y + h\n",
    "            boxes.append([x0, y0, x1, y1])\n",
    "            labels.append(int(a[\"category_id\"]))\n",
    "            m = np.zeros((H, W), dtype=np.uint8)\n",
    "            m[int(y0):int(y1), int(x0):int(x1)] = 1\n",
    "            masks.append(m)\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes  = [[0.0, 0.0, 1.0, 1.0]]\n",
    "            labels = [0]\n",
    "            masks  = [np.zeros((H, W), dtype=np.uint8)]\n",
    "\n",
    "        boxes  = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        masks  = torch.as_tensor(np.stack(masks, 0), dtype=torch.uint8)\n",
    "\n",
    "        # --- light data augments for training only ---\n",
    "        if self.is_train:\n",
    "            # Horizontal flip with proper bbox/mask update\n",
    "            if random.random() < 0.5:\n",
    "                img   = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                # flip boxes: x' = W - x\n",
    "                x0, y0, x1, y1 = boxes[:, 0].clone(), boxes[:, 1], boxes[:, 2].clone(), boxes[:, 3]\n",
    "                boxes[:, 0] = W - x1\n",
    "                boxes[:, 2] = W - x0\n",
    "                masks = torch.flip(masks, dims=[2])  # flip width dimension\n",
    "\n",
    "            # mild brightness/contrast jitter\n",
    "            img_t = TF.to_tensor(img)\n",
    "            # contrast\n",
    "            c = 1.0 + (random.random() - 0.5) * 0.4  # ±20%\n",
    "            img_t = torch.clamp((img_t - 0.5) * c + 0.5, 0, 1)\n",
    "            # brightness\n",
    "            b = 1.0 + (random.random() - 0.5) * 0.4\n",
    "            img_t = torch.clamp(img_t * b, 0, 1)\n",
    "            img   = TF.to_pil_image(img_t)\n",
    "\n",
    "        # --- sanitize after any augments ---\n",
    "        boxes, labels, masks = self._sanitize(boxes, labels, masks, W, H, min_size=1.0)\n",
    "\n",
    "        image = TF.to_tensor(img)\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "            \"image_id\": torch.tensor([im_id]),\n",
    "            \"iscrowd\": torch.zeros((boxes.shape[0],), dtype=torch.int64),\n",
    "            \"area\": (boxes[:,2]-boxes[:,0]) * (boxes[:,3]-boxes[:,1]),\n",
    "        }\n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, tgts = zip(*batch)\n",
    "    return list(imgs), list(tgts)\n",
    "\n",
    "# ---- DataLoaders ----\n",
    "train_ds = COCOSimple(train_json, train_imgdir, is_train=True)\n",
    "val_ds   = COCOSimple(val_json,   val_imgdir,   is_train=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True,  num_workers=2, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "# ---- Model ----\n",
    "model = maskrcnn_resnet50_fpn(weights=\"COCO_V1\")\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=2.5e-4, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "\n",
    "# ---- Train for 30 epochs, save to a NEW output dir ----\n",
    "EPOCHS = 30\n",
    "save_dir = OUT / \"tv_maskrcnn_output_e30\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Train {epoch+1}/{EPOCHS}\"):\n",
    "        images  = [im.to(device) for im in images]\n",
    "        targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    lr_scheduler.step()\n",
    "    train_loss /= max(1, len(train_loader))\n",
    "\n",
    "    # val (compute losses)\n",
    "    model.train()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc=\"Val\"):\n",
    "            images  = [im.to(device) for im in images]\n",
    "            targets = [{k:v.to(device) for k,v in t.items()} for t in targets]\n",
    "            ldict = model(images, targets)\n",
    "            val_loss += sum(ldict.values()).item()\n",
    "    val_loss /= max(1, len(val_loader))\n",
    "    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}  val_loss={val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(model.state_dict(), str(save_dir / \"model_best.pth\"))\n",
    "        print(\"✅ Saved best:\", save_dir / \"model_best.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), str(save_dir / \"model_last.pth\"))\n",
    "print(\"✔ Training complete →\", save_dir / \"model_last.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e12d83ef-a51d-4515-a170-bd5efd3f0ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-24251d5803/.local/lib/python3.10/site-packages/torchvision/utils.py:352: UserWarning: boxes doesn't contain any box. No box was drawn\n",
      "  warnings.warn(\"boxes doesn't contain any box. No box was drawn\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ wrote previews to: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/val_preds\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "label_names = [\"__bg__\"] + TARGETS\n",
    "pred_dir = OUT / \"val_preds\"\n",
    "pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i in range(min(12, len(val_ds))):\n",
    "    img, _ = val_ds[i]\n",
    "    with torch.no_grad():\n",
    "        pred = model([img.to(device)])[0]\n",
    "    boxes  = pred[\"boxes\"].cpu()\n",
    "    labels = pred[\"labels\"].cpu()\n",
    "    scores = pred[\"scores\"].cpu()\n",
    "\n",
    "    keep = scores > 0.5\n",
    "    boxes, labels, scores = boxes[keep], labels[keep], scores[keep]\n",
    "\n",
    "    drawn = draw_bounding_boxes(\n",
    "        (img*255).to(torch.uint8),\n",
    "        boxes,\n",
    "        [f\"{label_names[int(l)]}:{float(s):.2f}\" for l,s in zip(labels, scores)],\n",
    "        width=2\n",
    "    )\n",
    "    Image.fromarray(drawn.permute(1,2,0).numpy()).save(pred_dir/f\"val_{i:04d}.jpg\")\n",
    "\n",
    "print(\"✔ wrote previews to:\", pred_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc8f56c-e343-402c-ac30-a4b8684f46cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer val: 100%|██████████| 97/97 [00:20<00:00,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ wrote predictions: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/pred_val.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, json, torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "\n",
    "BASE = Path(\"/home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout\")\n",
    "OUT  = BASE / \"MaskRCNN_torchvision\"\n",
    "save_dir = OUT / \"tv_maskrcnn_output_e30\"\n",
    "\n",
    "val_json   = str(OUT / \"validation.json\")\n",
    "val_imgdir = str(BASE / \"images\" / \"validation\")\n",
    "\n",
    "TARGETS = [\"TEXT\",\"TABLE\",\"PICTURE\",\"CAPTION\"]\n",
    "NUM_CLASSES = 1 + len(TARGETS)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- rebuild model and load best weights ---\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "model = maskrcnn_resnet50_fpn(weights=\"COCO_V1\")\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(save_dir / \"model_best.pth\", map_location=\"cpu\"))\n",
    "model.to(device).eval()\n",
    "\n",
    "# --- load COCO val to iterate in order ---\n",
    "with open(val_json, \"r\") as f: coco_val = json.load(f)\n",
    "images = coco_val[\"images\"]\n",
    "\n",
    "SCORE_TH = 0.5  # adjust if you want more/less detections\n",
    "preds_for_coco = []  # list of dicts: {image_id, category_id, bbox, score}\n",
    "\n",
    "for im in tqdm(images, desc=\"Infer val\"):\n",
    "    path = os.path.join(val_imgdir, im[\"file_name\"])\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    tensor = TF.to_tensor(img).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model([tensor])[0]\n",
    "\n",
    "    boxes  = out[\"boxes\"].cpu()\n",
    "    labels = out[\"labels\"].cpu()\n",
    "    scores = out[\"scores\"].cpu()\n",
    "\n",
    "    keep = scores > SCORE_TH\n",
    "    boxes, labels, scores = boxes[keep], labels[keep], scores[keep]\n",
    "\n",
    "    # xyxy -> xywh\n",
    "    if len(boxes):\n",
    "        xywh = boxes.clone()\n",
    "        xywh[:,2] = boxes[:,2] - boxes[:,0]\n",
    "        xywh[:,3] = boxes[:,3] - boxes[:,1]\n",
    "        xywh[:,0] = boxes[:,0]\n",
    "        xywh[:,1] = boxes[:,1]\n",
    "        for b, lab, sc in zip(xywh.tolist(), labels.tolist(), scores.tolist()):\n",
    "            preds_for_coco.append({\n",
    "                \"image_id\": im[\"id\"],\n",
    "                \"category_id\": int(lab),   # already 1..4\n",
    "                \"bbox\": [float(b[0]), float(b[1]), float(b[2]), float(b[3])],\n",
    "                \"score\": float(sc)\n",
    "            })\n",
    "\n",
    "pred_path = OUT / \"pred_val.json\"\n",
    "with open(pred_path, \"w\") as f: json.dump(preds_for_coco, f)\n",
    "print(\"✓ wrote predictions:\", pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7be40ff-f9af-4c9b-a702-0b81d98f2fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed headers in: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/validation.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"/home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout\")\n",
    "OUT  = BASE / \"MaskRCNN_torchvision\"\n",
    "\n",
    "def ensure_coco_headers(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    # add minimal headers if missing\n",
    "    data.setdefault(\"info\", {\n",
    "        \"description\": \"DocLayNet 4-class (TEXT/TABLE/PICTURE/CAPTION)\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"year\": 2025,\n",
    "        \"contributor\": \"auto-converted\",\n",
    "        \"date_created\": \"\"\n",
    "    })\n",
    "    data.setdefault(\"licenses\", [])\n",
    "    # (optional) ensure required top-level keys exist\n",
    "    data.setdefault(\"images\", [])\n",
    "    data.setdefault(\"annotations\", [])\n",
    "    data.setdefault(\"categories\", [])\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "    print(\"Fixed headers in:\", json_path)\n",
    "\n",
    "ensure_coco_headers(str(OUT / \"validation.json\"))\n",
    "# (optional) ensure_coco_headers(str(OUT / \"train.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e5cdce-f807-4c3a-b8e5-da7bd3f90676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.98s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.23s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.078\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.023\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.042\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.110\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.156\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.168\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json\n",
    "\n",
    "val_json = str(OUT / \"validation.json\")\n",
    "pred_path = str(OUT / \"pred_val.json\")  # from the inference step\n",
    "\n",
    "gt = COCO(val_json)\n",
    "dt = gt.loadRes(pred_path)\n",
    "\n",
    "evaluator = COCOeval(gt, dt, iouType=\"bbox\")\n",
    "evaluator.evaluate()\n",
    "evaluator.accumulate()\n",
    "evaluator.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b51eaa-4e9c-464e-8ef8-243c84ac96e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer (no-threshold): 100%|██████████| 97/97 [00:14<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/pred_val.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "BASE = \"/home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout\"\n",
    "OUT  = f\"{BASE}/MaskRCNN_torchvision\"\n",
    "val_json   = f\"{OUT}/validation.json\"\n",
    "val_imgdir = f\"{BASE}/images/validation\"\n",
    "best_ckpt  = f\"{OUT}/tv_maskrcnn_output_e30/model_best.pth\"\n",
    "\n",
    "TARGETS = [\"TEXT\",\"TABLE\",\"PICTURE\",\"CAPTION\"]\n",
    "NUM_CLASSES = 1 + len(TARGETS)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# rebuild model\n",
    "model = maskrcnn_resnet50_fpn(weights=\"COCO_V1\")\n",
    "in_feat = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feat, NUM_CLASSES)\n",
    "in_feat_m = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_feat_m, 256, NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(best_ckpt, map_location=\"cpu\"))\n",
    "model.to(device).eval()\n",
    "\n",
    "with open(val_json,\"r\") as f: coco_val = json.load(f)\n",
    "images = coco_val[\"images\"]\n",
    "\n",
    "preds = []\n",
    "for im in tqdm(images, desc=\"Infer (no-threshold)\"):\n",
    "    path = os.path.join(val_imgdir, im[\"file_name\"])\n",
    "    img  = Image.open(path).convert(\"RGB\")\n",
    "    tens = TF.to_tensor(img).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model([tens])[0]\n",
    "    # sort by score desc and keep top 100\n",
    "    scores = out[\"scores\"].cpu()\n",
    "    order  = torch.argsort(scores, descending=True)[:100]\n",
    "    boxes  = out[\"boxes\"].cpu()[order]\n",
    "    labels = out[\"labels\"].cpu()[order]\n",
    "    scores = scores[order]\n",
    "\n",
    "    # xyxy → xywh\n",
    "    xywh = boxes.clone()\n",
    "    xywh[:,2] = boxes[:,2] - boxes[:,0]\n",
    "    xywh[:,3] = boxes[:,3] - boxes[:,1]\n",
    "    xywh[:,0] = boxes[:,0]\n",
    "    xywh[:,1] = boxes[:,1]\n",
    "\n",
    "    for b, lab, sc in zip(xywh.tolist(), labels.tolist(), scores.tolist()):\n",
    "        preds.append({\n",
    "            \"image_id\": im[\"id\"],\n",
    "            \"category_id\": int(lab),           # MUST be 1..4\n",
    "            \"bbox\": [float(b[0]), float(b[1]), float(b[2]), float(b[3])],  # pixels\n",
    "            \"score\": float(sc)\n",
    "        })\n",
    "\n",
    "pred_path = f\"{OUT}/pred_val.json\"\n",
    "with open(pred_path, \"w\") as f: json.dump(preds, f)\n",
    "print(\"wrote:\", pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c755b5e-4f4a-4152-9dc4-b77ad346372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote overlays to: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/vis_gt_pred\n"
     ]
    }
   ],
   "source": [
    "import os, json, torch\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "val_json = f\"{OUT}/validation.json\"\n",
    "with open(val_json,\"r\") as f: coco_val=json.load(f)\n",
    "by_img = {im[\"id\"]: im for im in coco_val[\"images\"]}\n",
    "gt_by_img = {}\n",
    "for ann in coco_val[\"annotations\"]:\n",
    "    gt_by_img.setdefault(ann[\"image_id\"], []).append(ann)\n",
    "\n",
    "# pick 5 random images\n",
    "import random\n",
    "ids = random.sample(list(by_img.keys()), k=min(5, len(by_img)))\n",
    "ID2NAME = {1:\"TEXT\",2:\"TABLE\",3:\"PICTURE\",4:\"CAPTION\"}\n",
    "\n",
    "with open(f\"{OUT}/pred_val.json\",\"r\") as f: preds = json.load(f)\n",
    "preds_map = {}\n",
    "for p in preds:\n",
    "    preds_map.setdefault(p[\"image_id\"], []).append(p)\n",
    "\n",
    "os.makedirs(f\"{OUT}/vis_gt_pred\", exist_ok=True)\n",
    "for im_id in ids:\n",
    "    meta = by_img[im_id]\n",
    "    path = os.path.join(val_imgdir, meta[\"file_name\"])\n",
    "    img  = Image.open(path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(img, \"RGBA\")\n",
    "\n",
    "    # draw GT in green\n",
    "    for a in gt_by_img.get(im_id, []):\n",
    "        x,y,w,h = a[\"bbox\"]\n",
    "        draw.rectangle([x,y,x+w,y+h], outline=(0,255,0,255), width=3)\n",
    "        draw.text((x,y), f\"GT:{ID2NAME[a['category_id']]}\", fill=(0,255,0,255))\n",
    "\n",
    "    # draw Pred in red\n",
    "    for p in preds_map.get(im_id, []):\n",
    "        x,y,w,h = p[\"bbox\"]\n",
    "        draw.rectangle([x,y,x+w,y+h], outline=(255,0,0,255), width=2)\n",
    "        draw.text((x,y+h), f\"P:{ID2NAME.get(p['category_id'],'?')} {p['score']:.2f}\", fill=(255,0,0,255))\n",
    "\n",
    "    img.save(f\"{OUT}/vis_gt_pred/{meta['file_name']}\")\n",
    "print(\"Wrote overlays to:\", f\"{OUT}/vis_gt_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "916c1b46-4f3b-44e0-ac80-680b47cc2ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved cleaned overlays (GT=green, Pred=red) to: /home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout/MaskRCNN_torchvision/vis_gt_pred_nms\n"
     ]
    }
   ],
   "source": [
    "import os, json, random, torch\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.ops import nms\n",
    "\n",
    "# ---- Paths (adjust BASE if yours differs) ----\n",
    "BASE = Path(\"/home/jupyter-24251d5803/DLS_dataset/DocLayNet-Balanced-Layout\")\n",
    "OUT  = BASE / \"MaskRCNN_torchvision\"\n",
    "save_dir = OUT / \"tv_maskrcnn_output_e30\"\n",
    "val_json   = OUT / \"validation.json\"\n",
    "val_imgdir = BASE / \"images\" / \"validation\"\n",
    "vis_dir    = OUT / \"vis_gt_pred_nms\"\n",
    "vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Labels ----\n",
    "TARGETS = [\"TEXT\",\"TABLE\",\"PICTURE\",\"CAPTION\"]\n",
    "ID2NAME = {1:\"TEXT\", 2:\"TABLE\", 3:\"PICTURE\", 4:\"CAPTION\"}\n",
    "\n",
    "# ---- Load model (best) ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = maskrcnn_resnet50_fpn(weights=\"COCO_V1\")\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 1 + len(TARGETS))\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, 1 + len(TARGETS))\n",
    "model.load_state_dict(torch.load(save_dir / \"model_best.pth\", map_location=\"cpu\"))\n",
    "model.to(device).eval()\n",
    "\n",
    "# ---- Load GT COCO for overlays ----\n",
    "with open(val_json, \"r\") as f:\n",
    "    coco_val = json.load(f)\n",
    "images = coco_val[\"images\"]\n",
    "gt_by_img = {}\n",
    "for ann in coco_val[\"annotations\"]:\n",
    "    gt_by_img.setdefault(ann[\"image_id\"], []).append(ann)\n",
    "\n",
    "# ---- Viz settings ----\n",
    "CONF_TH = 0.5       # score threshold\n",
    "NMS_IOU = 0.5       # NMS IoU threshold\n",
    "NUM_SAMPLES = min(10, len(images))\n",
    "\n",
    "# ---- Draw helper ----\n",
    "def draw_box(draw, xywh, color, text=None, width=3):\n",
    "    x,y,w,h = xywh\n",
    "    draw.rectangle([x,y,x+w,y+h], outline=color, width=width)\n",
    "    if text:\n",
    "        draw.text((x, max(0, y-12)), text, fill=color)\n",
    "\n",
    "# ---- Run on a few random images ----\n",
    "for im in random.sample(images, NUM_SAMPLES):\n",
    "    img_path = val_imgdir / im[\"file_name\"]\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    # Predict\n",
    "    tens = TF.to_tensor(img).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model([tens])[0]\n",
    "\n",
    "    boxes_xyxy = out[\"boxes\"].cpu()\n",
    "    labels     = out[\"labels\"].cpu()\n",
    "    scores     = out[\"scores\"].cpu()\n",
    "\n",
    "    # Filter by score\n",
    "    keep = scores > CONF_TH\n",
    "    boxes_xyxy = boxes_xyxy[keep]\n",
    "    labels     = labels[keep]\n",
    "    scores     = scores[keep]\n",
    "\n",
    "    # Apply NMS\n",
    "    if len(boxes_xyxy) > 0:\n",
    "        keep_n = nms(boxes_xyxy, scores, NMS_IOU)\n",
    "        boxes_xyxy = boxes_xyxy[keep_n]\n",
    "        labels     = labels[keep_n]\n",
    "        scores     = scores[keep_n]\n",
    "\n",
    "    # Convert xyxy -> xywh for drawing convenience\n",
    "    boxes_xywh = []\n",
    "    for b in boxes_xyxy.tolist():\n",
    "        x0,y0,x1,y1 = b\n",
    "        boxes_xywh.append([x0, y0, x1-x0, y1-y0])\n",
    "\n",
    "    # Draw GT (green) + Pred (red)\n",
    "    canvas = img.copy()\n",
    "    draw = ImageDraw.Draw(canvas, \"RGBA\")\n",
    "\n",
    "    # GT in green\n",
    "    for a in gt_by_img.get(im[\"id\"], []):\n",
    "        x,y,w,h = a[\"bbox\"]\n",
    "        draw_box(draw, [x,y,w,h], color=(0,255,0,255), text=f\"GT:{ID2NAME.get(a['category_id'],'?')}\", width=3)\n",
    "\n",
    "    # Pred in red (after threshold+NMS)\n",
    "    for (x,y,w,h), lab, sc in zip(boxes_xywh, labels.tolist(), scores.tolist()):\n",
    "        draw_box(draw, [x,y,w,h], color=(255,0,0,255), text=f\"P:{ID2NAME.get(lab,'?')} {sc:.2f}\", width=2)\n",
    "\n",
    "    out_path = vis_dir / im[\"file_name\"]\n",
    "    canvas.save(out_path)\n",
    "\n",
    "print(\"✓ Saved cleaned overlays (GT=green, Pred=red) to:\", vis_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376c555-c683-4588-8faa-a9e29a52b36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
